{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5439f6-5342-4ce2-8e1c-d1bbd138cdea",
   "metadata": {},
   "outputs": [
    {
     "ename": "DeprecationError",
     "evalue": "PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDeprecationError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(pdf_text)\n\u001b[0;32m     18\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhydin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCRF\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCRFAI.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your actual PDF file path\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m extracted_text \u001b[38;5;241m=\u001b[39m extract_text_from_pdf(pdf_path)\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mextract_text_from_pdf\u001b[1;34m(pdf_file)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_pdf\u001b[39m(pdf_file: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(pdf_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# PyPDF2 is the correct library with PdfFileReader\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m         reader \u001b[38;5;241m=\u001b[39m PyPDF2\u001b[38;5;241m.\u001b[39mPdfFileReader(pdf, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m         pdf_text \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(reader\u001b[38;5;241m.\u001b[39mnumPages):\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;66;03m# Get the page object\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PyPDF2\\_reader.py:1974\u001b[0m, in \u001b[0;36mPdfFileReader.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1974\u001b[0m     deprecation_with_replacement(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPdfFileReader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPdfReader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1976\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# maintain the default\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PyPDF2\\_utils.py:369\u001b[0m, in \u001b[0;36mdeprecation_with_replacement\u001b[1;34m(old_name, new_name, removed_in)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeprecation_with_replacement\u001b[39m(\n\u001b[0;32m    364\u001b[0m     old_name: \u001b[38;5;28mstr\u001b[39m, new_name: \u001b[38;5;28mstr\u001b[39m, removed_in: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    366\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    Raise an exception that a feature was already removed, but has a replacement.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     deprecation(DEPR_MSG_HAPPENED\u001b[38;5;241m.\u001b[39mformat(old_name, removed_in, new_name))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PyPDF2\\_utils.py:351\u001b[0m, in \u001b[0;36mdeprecation\u001b[1;34m(msg)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeprecation\u001b[39m(msg: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DeprecationError(msg)\n",
      "\u001b[1;31mDeprecationError\u001b[0m: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead."
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_file: str) -> str:\n",
    "    with open(pdf_file, 'rb') as pdf:\n",
    "        # PyPDF2 is the correct library with PdfFileReader\n",
    "        reader = PyPDF2.PdfFileReader(pdf, strict=False)\n",
    "        pdf_text = []\n",
    "\n",
    "        for page_num in range(reader.numPages):\n",
    "            # Get the page object\n",
    "            page = reader.getPage(page_num)\n",
    "            # Extract text from the page\n",
    "            content = page.extractText()\n",
    "            pdf_text.append(content)\n",
    "            \n",
    "    return \"\\n\".join(pdf_text)\n",
    "\n",
    "pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"  # Replace with your actual PDF file path\n",
    "extracted_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7b6cf9-5676-4af3-948a-8d72d41e264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_file: str) -> str:\n",
    "    with open(pdf_file, 'rb') as pdf:\n",
    "        # Use PdfReader instead of PdfFileReader (which is deprecated)\n",
    "        reader = PyPDF2.PdfReader(pdf, strict=False)\n",
    "        pdf_text = []\n",
    "\n",
    "        # Update to use len(reader.pages) instead of numPages\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            # Get the page object - updated syntax\n",
    "            page = reader.pages[page_num]\n",
    "            # Extract text from the page - method name remains the same\n",
    "            content = page.extract_text()  # Updated from extractText() to extract_text()\n",
    "            pdf_text.append(content)\n",
    "            \n",
    "    return \"\\n\".join(pdf_text)\n",
    "\n",
    "pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"  # Replace with your actual PDF file path\n",
    "extracted_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5602fd6-79e0-4521-beed-beb29429fb55",
   "metadata": {},
   "source": [
    "# Assistant\n",
    "The error occurs because `PdfFileReader` is deprecated and was removed in PyPDF2 version 3.0.0. You need to use `PdfReader` instead, which is the updated class name in newer versions of PyPDF2.\n",
    "\n",
    "Would you like me to provide the corrected code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460336d-31c1-481f-857a-a1982b029b64",
   "metadata": {},
   "source": [
    "# User\n",
    "yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2849ec08-63c8-4f85-8cf7-25d690329800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assistant\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_file: str) -> str:\n",
    "    with open(pdf_file, 'rb') as pdf:\n",
    "        # Use PdfReader instead of PdfFileReader (which is deprecated)\n",
    "        reader = PyPDF2.PdfReader(pdf, strict=False)\n",
    "        pdf_text = []\n",
    "\n",
    "        # Update to use len(reader.pages) instead of numPages\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            # Get the page object - updated syntax\n",
    "            page = reader.pages[page_num]\n",
    "            # Extract text from the page - method name remains the same\n",
    "            content = page.extract_text()  # Updated from extractText() to extract_text()\n",
    "            pdf_text.append(content)\n",
    "            \n",
    "    return \"\\n\".join(pdf_text)\n",
    "\n",
    "pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"  # Replace with your actual PDF file path\n",
    "extracted_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dac41595-7309-4a5d-8b7c-9c77bb024da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(pdf_file:str) ->str:\n",
    "    reader=PdfReader(pdf_file)\n",
    "    text=\"\" \n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            text+=page.extract_text()\n",
    "    return text\n",
    "pdf_path=r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path) #before you sleep set a alarm to remind yourself \n",
    "#to get rid of ALL of your thoughts fro the REST of your life and put phone in the desk drawers \n",
    "# and ONLY use insta on phone at the gym no where else even at church "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8943b02-be5c-4baf-8840-725ba97ac1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (3.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_chunks(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    step = chunk_size - overlap\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += step\n",
    "    return chunks\n",
    "🎯 Example Usage\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "pdf_path = \"your_pdf_file.pdf\"  # replace with your actual path\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "chunks = create_chunks(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):  # just show first 3 chunks\n",
    "    print(f\"--- Chunk {i+1} ---\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ca47aa4-7f97-4cb4-a661-6422307febca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chunk 1 ---\n",
      "9.0 RK 15NOV2021 PROD: Unique CRFs\n",
      "Form: Demographics\n",
      "Generated On: 13 DEC 2021 17:19:35\n",
      "9.0 RK 15NOV2021 PROD\n",
      "(8071)Date of Birth Fixed Unit: \n",
      "dd MMM yyyy\n",
      "Age\n",
      "Sex Male\n",
      "Female\n",
      "Ethnicity Hispanic or Latino\n",
      "Not Hispanic or Latino\n",
      "Unknown\n",
      "Not Applicable\n",
      "Race\n",
      "Asian\n",
      "Black or African American\n",
      "American Indian or Alaska Native\n",
      "Native Hawaiian or Other Pacific Islander\n",
      "White\n",
      "Other\n",
      "Race is 'Other', please specify\n",
      "3 of 499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(pdf_file:str) ->str:\n",
    "    reader=PdfReader(pdf_file)\n",
    "    text=\"\" \n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            text+=page.extract_text()\n",
    "    return text\n",
    "pdf_path=r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path) #before you sleep set a alarm to remind yourself \n",
    "#to get rid of ALL of your thoughts fro the REST of your life and put phone in the desk drawers \n",
    "# and ONLY use insta on phone at the gym no where else even at church \n",
    "def create_chunks(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    step = chunk_size - overlap\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += step\n",
    "    return chunks\n",
    "pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"  # replace with your actual path\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "chunks = create_chunks(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):  # just show first 3 chunks\n",
    "    print(f\"--- Chunk {i+1} ---\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07704666-e6c5-42d2-9d0d-4024e516a537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38b3da25-3315-4fc4-a399-24c476454fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openaiNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading openai-1.88.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\hydin\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hydin\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.88.0-py3-none-any.whl (734 kB)\n",
      "   ---------------------------------------- 0.0/734.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/734.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/734.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/734.3 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/734.3 kB ? eta -:--:--\n",
      "   --------------------------- ---------- 524.3/734.3 kB 837.5 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 524.3/734.3 kB 837.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 734.3/734.3 kB 702.6 kB/s eta 0:00:00\n",
      "Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl (206 kB)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/15.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/15.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/15.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.0 MB 430.4 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.5/15.0 MB 430.4 kB/s eta 0:00:34\n",
      "   -- ------------------------------------- 0.8/15.0 MB 441.7 kB/s eta 0:00:33\n",
      "   -- ------------------------------------- 0.8/15.0 MB 441.7 kB/s eta 0:00:33\n",
      "   -- ------------------------------------- 1.0/15.0 MB 493.7 kB/s eta 0:00:29\n",
      "   -- ------------------------------------- 1.0/15.0 MB 493.7 kB/s eta 0:00:29\n",
      "   -- ------------------------------------- 1.0/15.0 MB 493.7 kB/s eta 0:00:29\n",
      "   --- ------------------------------------ 1.3/15.0 MB 472.8 kB/s eta 0:00:29\n",
      "   --- ------------------------------------ 1.3/15.0 MB 472.8 kB/s eta 0:00:29\n",
      "   --- ------------------------------------ 1.3/15.0 MB 472.8 kB/s eta 0:00:29\n",
      "   --- ------------------------------------ 1.3/15.0 MB 472.8 kB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 1.6/15.0 MB 432.4 kB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 1.6/15.0 MB 432.4 kB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 1.6/15.0 MB 432.4 kB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 1.6/15.0 MB 432.4 kB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 1.6/15.0 MB 432.4 kB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 1.8/15.0 MB 382.8 kB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 1.8/15.0 MB 382.8 kB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 1.8/15.0 MB 382.8 kB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 1.8/15.0 MB 382.8 kB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 1.8/15.0 MB 382.8 kB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 1.8/15.0 MB 382.8 kB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 341.4 kB/s eta 0:00:38\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.4/15.0 MB 264.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------ --------------------------------- 2.6/15.0 MB 212.1 kB/s eta 0:00:59\n",
      "   ------- -------------------------------- 2.9/15.0 MB 182.8 kB/s eta 0:01:07\n",
      "   ------- -------------------------------- 2.9/15.0 MB 182.8 kB/s eta 0:01:07\n",
      "   ------- -------------------------------- 2.9/15.0 MB 182.8 kB/s eta 0:01:07\n",
      "   ------- -------------------------------- 2.9/15.0 MB 182.8 kB/s eta 0:01:07\n",
      "   ------- -------------------------------- 2.9/15.0 MB 182.8 kB/s eta 0:01:07\n",
      "   ------- -------------------------------- 2.9/15.0 MB 182.8 kB/s eta 0:01:07\n",
      "   ------- -------------------------------- 2.9/15.0 MB 182.8 kB/s eta 0:01:07\n",
      "   ------- -------------------------------- 2.9/15.0 MB 182.8 kB/s eta 0:01:07\n",
      "   ------- -------------------------------- 2.9/15.0 MB 182.8 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 3.1/15.0 MB 179.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 3.1/15.0 MB 179.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 3.1/15.0 MB 179.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 3.1/15.0 MB 179.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 3.1/15.0 MB 179.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 3.1/15.0 MB 179.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 3.1/15.0 MB 179.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 3.1/15.0 MB 179.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 3.1/15.0 MB 179.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 3.1/15.0 MB 179.0 kB/s eta 0:01:07\n",
      "   --------- ------------------------------ 3.4/15.0 MB 172.2 kB/s eta 0:01:08\n",
      "   --------- ------------------------------ 3.4/15.0 MB 172.2 kB/s eta 0:01:08\n",
      "   --------- ------------------------------ 3.4/15.0 MB 172.2 kB/s eta 0:01:08\n",
      "   --------- ------------------------------ 3.4/15.0 MB 172.2 kB/s eta 0:01:08\n",
      "   --------- ------------------------------ 3.4/15.0 MB 172.2 kB/s eta 0:01:08\n",
      "   --------- ------------------------------ 3.4/15.0 MB 172.2 kB/s eta 0:01:08\n",
      "   --------- ------------------------------ 3.7/15.0 MB 174.5 kB/s eta 0:01:05\n",
      "   --------- ------------------------------ 3.7/15.0 MB 174.5 kB/s eta 0:01:05\n",
      "   --------- ------------------------------ 3.7/15.0 MB 174.5 kB/s eta 0:01:05\n",
      "   --------- ------------------------------ 3.7/15.0 MB 174.5 kB/s eta 0:01:05\n",
      "   --------- ------------------------------ 3.7/15.0 MB 174.5 kB/s eta 0:01:05\n",
      "   ---------- ----------------------------- 3.9/15.0 MB 178.1 kB/s eta 0:01:03\n",
      "   ---------- ----------------------------- 3.9/15.0 MB 178.1 kB/s eta 0:01:03\n",
      "   ---------- ----------------------------- 3.9/15.0 MB 178.1 kB/s eta 0:01:03\n",
      "   ---------- ----------------------------- 3.9/15.0 MB 178.1 kB/s eta 0:01:03\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 183.7 kB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 4.5/15.0 MB 114.2 kB/s eta 0:01:33\n",
      "   ----------- ---------------------------- 4.5/15.0 MB 114.2 kB/s eta 0:01:33\n",
      "   ----------- ---------------------------- 4.5/15.0 MB 114.2 kB/s eta 0:01:33\n",
      "   ------------ --------------------------- 4.7/15.0 MB 113.8 kB/s eta 0:01:31\n",
      "   ------------ --------------------------- 4.7/15.0 MB 113.8 kB/s eta 0:01:31\n",
      "   ------------- -------------------------- 5.0/15.0 MB 115.1 kB/s eta 0:01:28\n",
      "   ------------- -------------------------- 5.0/15.0 MB 115.1 kB/s eta 0:01:28\n",
      "   ------------- -------------------------- 5.0/15.0 MB 115.1 kB/s eta 0:01:28\n",
      "   ------------- -------------------------- 5.2/15.0 MB 117.5 kB/s eta 0:01:24\n",
      "   ------------- -------------------------- 5.2/15.0 MB 117.5 kB/s eta 0:01:24\n",
      "   -------------- ------------------------- 5.5/15.0 MB 124.8 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 5.5/15.0 MB 124.8 kB/s eta 0:01:17\n",
      "   --------------- ------------------------ 5.8/15.0 MB 131.3 kB/s eta 0:01:11\n",
      "   --------------- ------------------------ 5.8/15.0 MB 131.3 kB/s eta 0:01:11\n",
      "   ---------------- ----------------------- 6.0/15.0 MB 135.5 kB/s eta 0:01:07\n",
      "   ---------------- ----------------------- 6.0/15.0 MB 135.5 kB/s eta 0:01:07\n",
      "   ---------------- ----------------------- 6.3/15.0 MB 142.2 kB/s eta 0:01:02\n",
      "   ---------------- ----------------------- 6.3/15.0 MB 142.2 kB/s eta 0:01:02\n",
      "   ---------------- ----------------------- 6.3/15.0 MB 142.2 kB/s eta 0:01:02\n",
      "   ----------------- ---------------------- 6.6/15.0 MB 152.0 kB/s eta 0:00:56\n",
      "   ----------------- ---------------------- 6.6/15.0 MB 152.0 kB/s eta 0:00:56\n",
      "   ----------------- ---------------------- 6.6/15.0 MB 152.0 kB/s eta 0:00:56\n",
      "   ----------------- ---------------------- 6.6/15.0 MB 152.0 kB/s eta 0:00:56\n",
      "   ----------------- ---------------------- 6.6/15.0 MB 152.0 kB/s eta 0:00:56\n",
      "   ------------------ --------------------- 6.8/15.0 MB 156.5 kB/s eta 0:00:53\n",
      "   ------------------ --------------------- 6.8/15.0 MB 156.5 kB/s eta 0:00:53\n",
      "   ------------------ --------------------- 6.8/15.0 MB 156.5 kB/s eta 0:00:53\n",
      "   ------------------ --------------------- 7.1/15.0 MB 162.1 kB/s eta 0:00:49\n",
      "   ------------------ --------------------- 7.1/15.0 MB 162.1 kB/s eta 0:00:49\n",
      "   ------------------ --------------------- 7.1/15.0 MB 162.1 kB/s eta 0:00:49\n",
      "   ------------------ --------------------- 7.1/15.0 MB 162.1 kB/s eta 0:00:49\n",
      "   ------------------ --------------------- 7.1/15.0 MB 162.1 kB/s eta 0:00:49\n",
      "   ------------------ --------------------- 7.1/15.0 MB 162.1 kB/s eta 0:00:49\n",
      "   ------------------- -------------------- 7.3/15.0 MB 173.7 kB/s eta 0:00:45\n",
      "   ------------------- -------------------- 7.3/15.0 MB 173.7 kB/s eta 0:00:45\n",
      "   ------------------- -------------------- 7.3/15.0 MB 173.7 kB/s eta 0:00:45\n",
      "   -------------------- ------------------- 7.6/15.0 MB 178.4 kB/s eta 0:00:42\n",
      "   -------------------- ------------------- 7.6/15.0 MB 178.4 kB/s eta 0:00:42\n",
      "   -------------------- ------------------- 7.6/15.0 MB 178.4 kB/s eta 0:00:42\n",
      "   -------------------- ------------------- 7.6/15.0 MB 178.4 kB/s eta 0:00:42\n",
      "   -------------------- ------------------- 7.6/15.0 MB 178.4 kB/s eta 0:00:42\n",
      "   -------------------- ------------------- 7.6/15.0 MB 178.4 kB/s eta 0:00:42\n",
      "   -------------------- ------------------- 7.6/15.0 MB 178.4 kB/s eta 0:00:42\n",
      "   -------------------- ------------------- 7.6/15.0 MB 178.4 kB/s eta 0:00:42\n",
      "   -------------------- ------------------- 7.6/15.0 MB 178.4 kB/s eta 0:00:42\n",
      "   -------------------- ------------------- 7.9/15.0 MB 176.4 kB/s eta 0:00:41\n",
      "   -------------------- ------------------- 7.9/15.0 MB 176.4 kB/s eta 0:00:41\n",
      "   -------------------- ------------------- 7.9/15.0 MB 176.4 kB/s eta 0:00:41\n",
      "   --------------------- ------------------ 8.1/15.0 MB 192.6 kB/s eta 0:00:36\n",
      "   --------------------- ------------------ 8.1/15.0 MB 192.6 kB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 8.4/15.0 MB 199.2 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 8.4/15.0 MB 199.2 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 8.4/15.0 MB 199.2 kB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 8.7/15.0 MB 205.1 kB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 8.9/15.0 MB 212.2 kB/s eta 0:00:29\n",
      "   ------------------------ --------------- 9.2/15.0 MB 219.5 kB/s eta 0:00:27\n",
      "   ------------------------ --------------- 9.2/15.0 MB 219.5 kB/s eta 0:00:27\n",
      "   ------------------------- -------------- 9.4/15.0 MB 225.7 kB/s eta 0:00:25\n",
      "   ------------------------- -------------- 9.7/15.0 MB 232.9 kB/s eta 0:00:23\n",
      "   ------------------------- -------------- 9.7/15.0 MB 232.9 kB/s eta 0:00:23\n",
      "   -------------------------- ------------- 10.0/15.0 MB 238.0 kB/s eta 0:00:22\n",
      "   -------------------------- ------------- 10.0/15.0 MB 238.0 kB/s eta 0:00:22\n",
      "   --------------------------- ------------ 10.2/15.0 MB 250.5 kB/s eta 0:00:20\n",
      "   --------------------------- ------------ 10.2/15.0 MB 250.5 kB/s eta 0:00:20\n",
      "   --------------------------- ------------ 10.5/15.0 MB 256.0 kB/s eta 0:00:18\n",
      "   ---------------------------- ----------- 10.7/15.0 MB 263.1 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 10.7/15.0 MB 263.1 kB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 11.0/15.0 MB 267.4 kB/s eta 0:00:15\n",
      "   ------------------------------ --------- 11.3/15.0 MB 274.3 kB/s eta 0:00:14\n",
      "   ------------------------------ --------- 11.3/15.0 MB 274.3 kB/s eta 0:00:14\n",
      "   ------------------------------ --------- 11.5/15.0 MB 290.9 kB/s eta 0:00:12\n",
      "   ------------------------------ --------- 11.5/15.0 MB 290.9 kB/s eta 0:00:12\n",
      "   ------------------------------- -------- 11.8/15.0 MB 296.3 kB/s eta 0:00:11\n",
      "   ------------------------------- -------- 11.8/15.0 MB 296.3 kB/s eta 0:00:11\n",
      "   ------------------------------- -------- 11.8/15.0 MB 296.3 kB/s eta 0:00:11\n",
      "   ------------------------------- -------- 11.8/15.0 MB 296.3 kB/s eta 0:00:11\n",
      "   -------------------------------- ------- 12.1/15.0 MB 297.8 kB/s eta 0:00:10\n",
      "   -------------------------------- ------- 12.1/15.0 MB 297.8 kB/s eta 0:00:10\n",
      "   -------------------------------- ------- 12.1/15.0 MB 297.8 kB/s eta 0:00:10\n",
      "   -------------------------------- ------- 12.3/15.0 MB 300.5 kB/s eta 0:00:09\n",
      "   -------------------------------- ------- 12.3/15.0 MB 300.5 kB/s eta 0:00:09\n",
      "   --------------------------------- ------ 12.6/15.0 MB 308.0 kB/s eta 0:00:08\n",
      "   --------------------------------- ------ 12.6/15.0 MB 308.0 kB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 12.8/15.0 MB 312.2 kB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 12.8/15.0 MB 312.2 kB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 12.8/15.0 MB 312.2 kB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 13.1/15.0 MB 316.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 13.1/15.0 MB 316.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 13.1/15.0 MB 316.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 13.1/15.0 MB 316.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 13.1/15.0 MB 316.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 13.1/15.0 MB 316.9 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 13.4/15.0 MB 313.7 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 13.4/15.0 MB 313.7 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 13.4/15.0 MB 313.7 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 13.4/15.0 MB 313.7 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 13.4/15.0 MB 313.7 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 13.4/15.0 MB 313.7 kB/s eta 0:00:06\n",
      "   ------------------------------------ --- 13.6/15.0 MB 448.6 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 13.6/15.0 MB 448.6 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 13.6/15.0 MB 448.6 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 13.9/15.0 MB 445.4 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 13.9/15.0 MB 445.4 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 13.9/15.0 MB 445.4 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 14.2/15.0 MB 445.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.2/15.0 MB 445.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.2/15.0 MB 445.0 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 14.4/15.0 MB 443.7 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 14.4/15.0 MB 443.7 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 14.4/15.0 MB 443.7 kB/s eta 0:00:02\n",
      "   ---------------------------------------  14.7/15.0 MB 445.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/15.0 MB 445.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  14.9/15.0 MB 445.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 445.9 kB/s eta 0:00:00\n",
      "Installing collected packages: jiter, faiss-cpu, openai\n",
      "\n",
      "   ------------- -------------------------- 1/3 [faiss-cpu]\n",
      "   ------------- -------------------------- 1/3 [faiss-cpu]\n",
      "   ------------- -------------------------- 1/3 [faiss-cpu]\n",
      "   ------------- -------------------------- 1/3 [faiss-cpu]\n",
      "   ------------- -------------------------- 1/3 [faiss-cpu]\n",
      "   ------------- -------------------------- 1/3 [faiss-cpu]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   ---------------------------------------- 3/3 [openai]\n",
      "\n",
      "Successfully installed faiss-cpu-1.11.0 jiter-0.10.0 openai-1.88.0\n"
     ]
    }
   ],
   "source": [
    "pip install openai faiss-cpu tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "144c3da9-afe1-454a-9d45-827592b3f815",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the date of birth format?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Call the function and capture the returned answer\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m answer \u001b[38;5;241m=\u001b[39m answer_question_from_pdf(pdf_path, question)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[1;32mIn[33], line 20\u001b[0m, in \u001b[0;36manswer_question_from_pdf\u001b[1;34m(pdf_path, question)\u001b[0m\n\u001b[0;32m     17\u001b[0m best_chunk \u001b[38;5;241m=\u001b[39m find_best_chunk(chunks, question)\n\u001b[0;32m     19\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse the following text to answer the question:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbest_chunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 20\u001b[0m api_key\u001b[38;5;241m=\u001b[39msk\u001b[38;5;241m-\u001b[39mproj\u001b[38;5;241m-\u001b[39mZ0Nl1JNlvsRSeyvCHjQ8l3G6HXH86uwP2Yjf25mRwtn4mHrIto7O30HOIktYJUnPaJmiShNcEkT3BlbkFJMsjMtmgLrbqrSD7O4hHQAXqPry3eXAOyaq3XBpaI8\u001b[38;5;241m-\u001b[39mAxUU9i5KrySspC5t56Wr7kGS2AlaZi4A\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Updated API call for OpenAI v1.0.0+\u001b[39;00m\n\u001b[0;32m     22\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI()  \u001b[38;5;66;03m# Initialize the client\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sk' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "\n",
    "# 4. Ask the question based on best chunk\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "\n",
    "    prompt = f\"Use the following text to answer the question:\\n\\n{best_chunk}\\n\\nQuestion: {question}\"\n",
    "    api_key=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\n",
    "    # Updated API call for OpenAI v1.0.0+\n",
    "    client = openai.OpenAI()  # Initialize the client\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Updated way to access the response content\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\" # Replace with the path to your actual PDF\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afdbb5f6-85b7-46f2-aca1-91de44bea9af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the date of birth format?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Call the function and capture the returned answer\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m answer \u001b[38;5;241m=\u001b[39m answer_question_from_pdf(pdf_path, question)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[1;32mIn[34], line 26\u001b[0m, in \u001b[0;36manswer_question_from_pdf\u001b[1;34m(pdf_path, question)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Updated API call for OpenAI v1.0.0+\u001b[39;00m\n\u001b[0;32m     25\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(api_key\u001b[38;5;241m=\u001b[39mapi_key)  \u001b[38;5;66;03m# Pass the API key to the client\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}],\n\u001b[0;32m     29\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Updated way to access the response content\u001b[39;00m\n\u001b[0;32m     33\u001b[0m answer \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    922\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    923\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    924\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    927\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    928\u001b[0m             {\n\u001b[0;32m    929\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    930\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    931\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    932\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    933\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    934\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    935\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    936\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    937\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    938\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    939\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    940\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    941\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    942\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    943\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    944\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    945\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    946\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    947\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    948\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    949\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    950\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    951\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    952\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    953\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    954\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    955\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    956\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    957\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    958\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    959\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m    960\u001b[0m             },\n\u001b[0;32m    961\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[0;32m    962\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m    963\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[0;32m    964\u001b[0m         ),\n\u001b[0;32m    965\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    966\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    967\u001b[0m         ),\n\u001b[0;32m    968\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    969\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    970\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    971\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1249\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1237\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1246\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1247\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[1;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1036\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "\n",
    "# 4. Ask the question based on best chunk\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "\n",
    "    prompt = f\"Use the following text to answer the question:\\n\\n{best_chunk}\\n\\nQuestion: {question}\"\n",
    "    \n",
    "    # Fix: Properly define the API key as a string\n",
    "    api_key = \"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\"\n",
    "    \n",
    "    # Updated API call for OpenAI v1.0.0+\n",
    "    client = openai.OpenAI(api_key=api_key)  # Pass the API key to the client\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Updated way to access the response content\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\" # Replace with the path to your actual PDF\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0c587af-3031-474c-a282-3fb426c31fd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the date of birth format?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Call the function and capture the returned answer\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m answer \u001b[38;5;241m=\u001b[39m answer_question_from_pdf(pdf_path, question)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[1;32mIn[35], line 28\u001b[0m, in \u001b[0;36manswer_question_from_pdf\u001b[1;34m(pdf_path, question)\u001b[0m\n\u001b[0;32m     25\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(api_key\u001b[38;5;241m=\u001b[39mapi_key)  \u001b[38;5;66;03m# Pass the API key to the client\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Changed model from \"gpt-4\" to \"gpt-3.5-turbo\" which is more widely available\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Using gpt-3.5-turbo instead of gpt-4\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}],\n\u001b[0;32m     31\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Updated way to access the response content\u001b[39;00m\n\u001b[0;32m     35\u001b[0m answer \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    922\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    923\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    924\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    927\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    928\u001b[0m             {\n\u001b[0;32m    929\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    930\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    931\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    932\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    933\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    934\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    935\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    936\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    937\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    938\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    939\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    940\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    941\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    942\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    943\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    944\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    945\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    946\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    947\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    948\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    949\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    950\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    951\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    952\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    953\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    954\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    955\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    956\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    957\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    958\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    959\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m    960\u001b[0m             },\n\u001b[0;32m    961\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[0;32m    962\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m    963\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[0;32m    964\u001b[0m         ),\n\u001b[0;32m    965\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    966\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    967\u001b[0m         ),\n\u001b[0;32m    968\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    969\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    970\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    971\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1249\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1237\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1246\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1247\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[1;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1036\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "\n",
    "# 4. Ask the question based on best chunk\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "\n",
    "    prompt = f\"Use the following text to answer the question:\\n\\n{best_chunk}\\n\\nQuestion: {question}\"\n",
    "    \n",
    "    # Fix: Properly define the API key as a string\n",
    "    api_key = \"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\"\n",
    "    \n",
    "    # Updated API call for OpenAI v1.0.0+\n",
    "    client = openai.OpenAI(api_key=api_key)  # Pass the API key to the client\n",
    "    \n",
    "    # Changed model from \"gpt-4\" to \"gpt-3.5-turbo\" which is more widely available\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Using gpt-3.5-turbo instead of gpt-4\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Updated way to access the response content\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\" # Replace with the path to your actual PDF\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23b1fa52-10bd-44bb-9d60-5b7b01413904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: An error occurred: name 'prompt' is not defined\n"
     ]
    }
   ],
   "source": [
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "\n",
    "# 4. Ask the question based on best chunk\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "\n",
    "    # Option 1: Use a different API key that has available quota\n",
    "    api_key = \"your-new-api-key-with-available-quota\"  # Replace with a valid API key\n",
    "    \n",
    "    # Option 2: Add error handling to gracefully handle rate limits\n",
    "    try:\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return answer\n",
    "    except openai.RateLimitError:\n",
    "        # Fallback option when API limit is reached\n",
    "        return f\"I couldn't process your question due to API limits. Here's the relevant text that might help:\\n\\n{best_chunk}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "232c1c05-9646-458a-9fa6-a74ed17fd539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: An error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-new*************************uota. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "\n",
    "# 4. Ask the question based on best chunk\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "\n",
    "    # Define the prompt here, before using it in the try block\n",
    "    prompt = f\"Use the following text to answer the question:\\n\\n{best_chunk}\\n\\nQuestion: {question}\"\n",
    "    \n",
    "    # Option 1: Use a different API key that has available quota\n",
    "    api_key = \"your-new-api-key-with-available-quota\"  # Replace with a valid API key\n",
    "    \n",
    "    # Option 2: Add error handling to gracefully handle rate limits\n",
    "    try:\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return answer\n",
    "    except openai.RateLimitError:\n",
    "        # Fallback option when API limit is reached\n",
    "        return f\"I couldn't process your question due to API limits. Here's the relevant text that might help:\\n\\n{best_chunk}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "319eb2ce-fe86-44c4-98e5-5c5da5f4c5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chunk 1 ---\n",
      "9.0 RK 15NOV2021 PROD: Unique CRFs\n",
      "Form: Demographics\n",
      "Generated On: 13 DEC 2021 17:19:35\n",
      "9.0 RK 15NOV2021 PROD\n",
      "(8071)Date of Birth Fixed Unit: \n",
      "dd MMM yyyy\n",
      "Age\n",
      "Sex Male\n",
      "Female\n",
      "Ethnicity Hispanic or Latino\n",
      "Not Hispanic or Latino\n",
      "Unknown\n",
      "Not Applicable\n",
      "Race\n",
      "Asian\n",
      "Black or African American\n",
      "American Indian or Alaska Native\n",
      "Native Hawaiian or Other Pacific Islander\n",
      "White\n",
      "Other\n",
      "Race is 'Other', please specify\n",
      "3 of 499\n",
      "\n",
      "Answer: An error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-your-*****************-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(pdf_file:str) ->str:\n",
    "    reader=PdfReader(pdf_file)\n",
    "    text=\"\" \n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            text+=page.extract_text()\n",
    "    return text\n",
    "pdf_path=r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path) #before you sleep set a alarm to remind yourself \n",
    "#to get rid of ALL of your thoughts fro the REST of your life and put phone in the desk drawers \n",
    "# and ONLY use insta on phone at the gym no where else even at church \n",
    "def create_chunks(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    step = chunk_size - overlap\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += step\n",
    "    return chunks\n",
    "pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"  # replace with your actual path\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "chunks = create_chunks(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):  # just show first 3 chunks\n",
    "    print(f\"--- Chunk {i+1} ---\\n{chunk}\\n\")\n",
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "\n",
    "# 4. Ask the question based on best chunk\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt = f\"Use the following text to answer the question:\\n\\n{best_chunk}\\n\\nQuestion: {question}\"\n",
    "    \n",
    "    # Option 1: Use your actual API key here\n",
    "    # You should ideally store this in an environment variable or config file\n",
    "    # rather than hardcoding it in your script\n",
    "    api_key = \"sk-your-actual-openai-api-key\"  # Replace with your actual API key\n",
    "    \n",
    "    # Option 2: Alternative approach - don't use the API if you're having quota issues\n",
    "    try:\n",
    "        # Only attempt API call if you have a valid key\n",
    "        if api_key.startswith(\"sk-\"):\n",
    "            client = openai.OpenAI(api_key=api_key)\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            return answer\n",
    "        else:\n",
    "            # If no valid API key is provided, return the relevant chunk instead\n",
    "            return f\"No valid API key provided. Here's the relevant text that might answer your question:\\n\\n{best_chunk}\"\n",
    "    except openai.RateLimitError:\n",
    "        # Fallback option when API limit is reached\n",
    "        return f\"I couldn't process your question due to API limits. Here's the relevant text that might help:\\n\\n{best_chunk}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0cd5b25e-51d6-41e9-9a6c-9c6f2bfe0eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the document, I found this information: 0 RK 15NOV2021 PROD\n",
      "(8071)Date of Birth Fixed Unit: \n",
      "dd MMM yyyy\n",
      "Age\n",
      "Sex Male\n",
      "Female\n",
      "Ethnicity Hispanic or Latino\n",
      "Not Hispanic or Latino\n",
      "Unknown\n",
      "Not Applicable\n",
      "Race\n",
      "Asian\n",
      "Black or African American\n",
      "American Indian or Alaska Native\n",
      "Native Hawaiian or Other Pacific Islander\n",
      "White\n",
      "Other\n",
      "Race is 'Other', please specify\n",
      "3 of 499\n"
     ]
    }
   ],
   "source": [
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "\n",
    "# 4. Ask the question based on best chunk - No OpenAI API required\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "    \n",
    "    # Simple keyword-based answering approach\n",
    "    question_lower = question.lower()\n",
    "    chunk_lower = best_chunk.lower()\n",
    "    \n",
    "    # Extract sentences from the chunk that might contain the answer\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]', best_chunk)\n",
    "    relevant_sentences = []\n",
    "    \n",
    "    # Look for sentences that contain keywords from the question\n",
    "    keywords = [word for word in question_lower.split() if len(word) > 3]\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # Check if the sentence contains any keywords from the question\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            relevant_sentences.append(sentence)\n",
    "    \n",
    "    if relevant_sentences:\n",
    "        answer = \" \".join(relevant_sentences)\n",
    "        return f\"Based on the document, I found this information: {answer}\"\n",
    "    else:\n",
    "        # If no specific sentences found, return the best chunk\n",
    "        return f\"I couldn't find a specific answer, but here's the most relevant section from the document:\\n\\n{best_chunk}\"\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f28e290d-d927-4b7a-948b-13d6dc042d52",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (2458043668.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[40], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    return text\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(pdf_file:str) ->str:\n",
    "    reader=PdfReader(pdf_file)\n",
    "    text=\"\" \n",
    "for page in reader.pages:\n",
    "    if page.extract_text():\n",
    "            text+=page.extract_text()\n",
    "return text\n",
    "pdf_path=r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path) #before you sleep set a alarm to remind yourself \n",
    "#to get rid of ALL of your thoughts fro the REST of your life and put phone in the desk drawers \n",
    "# and ONLY use insta on phone at the gym no where else even at church \n",
    "def create_chunks(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    step = chunk_size - overlap\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += step\n",
    "    return chunks\n",
    "pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"  # replace with your actual path\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "chunks = create_chunks(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):  # just show first 3 chunks\n",
    "    print(f\"--- Chunk {i+1} ---\\n{chunk}\\n\")\n",
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "\n",
    "# 4. Ask the question based on best chunk - No OpenAI API required\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "    \n",
    "    # Simple keyword-based answering approach\n",
    "    question_lower = question.lower()\n",
    "    chunk_lower = best_chunk.lower()\n",
    "    \n",
    "    # Extract sentences from the chunk that might contain the answer\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]', best_chunk)\n",
    "    relevant_sentences = []\n",
    "    \n",
    "    # Look for sentences that contain keywords from the question\n",
    "    keywords = [word for word in question_lower.split() if len(word) > 3]\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # Check if the sentence contains any keywords from the question\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            relevant_sentences.append(sentence)\n",
    "    \n",
    "    if relevant_sentences:\n",
    "        answer = \" \".join(relevant_sentences)\n",
    "        return f\"Based on the document, I found this information: {answer}\"\n",
    "    else:\n",
    "        # If no specific sentences found, return the best chunk\n",
    "        return f\"I couldn't find a specific answer, but here's the most relevant section from the document:\\n\\n{best_chunk}\"\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe6c274c-ce56-4241-9a3e-c4fac29ee4cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the date of birth format?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Call the function and capture the returned answer\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m answer \u001b[38;5;241m=\u001b[39m answer_question_from_pdf(pdf_path, question)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[1;32mIn[41], line 16\u001b[0m, in \u001b[0;36manswer_question_from_pdf\u001b[1;34m(pdf_path, question)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manswer_question_from_pdf\u001b[39m(pdf_path: \u001b[38;5;28mstr\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     15\u001b[0m     text \u001b[38;5;241m=\u001b[39m extract_text_from_pdf(pdf_path)\n\u001b[1;32m---> 16\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m create_chunks(text)\n\u001b[0;32m     17\u001b[0m     best_chunk \u001b[38;5;241m=\u001b[39m find_best_chunk(chunks, question)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Simple keyword-based answering approach\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 17\u001b[0m, in \u001b[0;36mcreate_chunks\u001b[1;34m(text, chunk_size, overlap)\u001b[0m\n\u001b[0;32m     15\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m step \u001b[38;5;241m=\u001b[39m chunk_size \u001b[38;5;241m-\u001b[39m overlap\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(text):\n\u001b[0;32m     18\u001b[0m     end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start \u001b[38;5;241m+\u001b[39m chunk_size, \u001b[38;5;28mlen\u001b[39m(text))\n\u001b[0;32m     19\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mappend(text[start:end])\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "def find_best_chunk(chunks, question):\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "\n",
    "# 4. Ask the question based on best chunk - No OpenAI API required\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "    \n",
    "    # Simple keyword-based answering approach\n",
    "    question_lower = question.lower()\n",
    "    chunk_lower = best_chunk.lower()\n",
    "    \n",
    "    # Extract sentences from the chunk that might contain the answer\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]', best_chunk)\n",
    "    relevant_sentences = []\n",
    "    \n",
    "    # Look for sentences that contain keywords from the question\n",
    "    keywords = [word for word in question_lower.split() if len(word) > 3]\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # Check if the sentence contains any keywords from the question\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            relevant_sentences.append(sentence)\n",
    "    \n",
    "    if relevant_sentences:\n",
    "        answer = \" \".join(relevant_sentences)\n",
    "        return f\"Based on the document, I found this information: {answer}\"\n",
    "    else:\n",
    "        # If no specific sentences found, return the best chunk\n",
    "        return f\"I couldn't find a specific answer, but here's the most relevant section from the document:\\n\\n{best_chunk}\"\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca63d85e-da0a-4fe2-9e6f-14eb796bc5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Failed to extract text from the PDF.\n"
     ]
    }
   ],
   "source": [
    "def find_best_chunk(chunks, question):\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    \n",
    "    # Check if chunks or question is None\n",
    "    if not chunks or question is None:\n",
    "        return \"No relevant information found.\"\n",
    "    \n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        # Check if chunk is None or empty\n",
    "        if not chunk:\n",
    "            continue\n",
    "            \n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    \n",
    "    # Return a default message if no good chunk was found\n",
    "    if not best_chunk:\n",
    "        return \"No relevant information found.\"\n",
    "    \n",
    "    return best_chunk\n",
    "\n",
    "# 4. Ask the question based on best chunk - No OpenAI API required\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Check if text extraction was successful\n",
    "    if not text:\n",
    "        return \"Failed to extract text from the PDF.\"\n",
    "        \n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "    \n",
    "    # Check if best_chunk is a default message\n",
    "    if best_chunk == \"No relevant information found.\" or not best_chunk:\n",
    "        return best_chunk\n",
    "    \n",
    "    # Simple keyword-based answering approach\n",
    "    question_lower = question.lower()\n",
    "    chunk_lower = best_chunk.lower()\n",
    "    \n",
    "    # Extract sentences from the chunk that might contain the answer\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]', best_chunk)\n",
    "    relevant_sentences = []\n",
    "    \n",
    "    # Look for sentences that contain keywords from the question\n",
    "    keywords = [word for word in question_lower.split() if len(word) > 3]\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # Check if the sentence contains any keywords from the question\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            relevant_sentences.append(sentence)\n",
    "    \n",
    "    if relevant_sentences:\n",
    "        answer = \" \".join(relevant_sentences)\n",
    "        return f\"Based on the document, I found this information: {answer}\"\n",
    "    else:\n",
    "        # If no specific sentences found, return the best chunk\n",
    "        return f\"I couldn't find a specific answer, but here's the most relevant section from the document:\\n\\n{best_chunk}\"\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5319542b-4728-4968-889b-52c732d4bc8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (706410841.py, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[43], line 68\u001b[1;36m\u001b[0m\n\u001b[1;33m    sentences = re.split(r'[.!?]', best_chunk)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(pdf_file:str) ->str:\n",
    "    reader=PdfReader(pdf_file)\n",
    "    text=\"\" \n",
    "for page in reader.pages:\n",
    "    if page.extract_text():\n",
    "            text+=page.extract_text()\n",
    "return text\n",
    "pdf_path=r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path) #before you sleep set a alarm to remind yourself \n",
    "#to get rid of ALL of your thoughts fro the REST of your life and put phone in the desk drawers \n",
    "# and ONLY use insta on phone at the gym no where else even at church \n",
    "def create_chunks(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    step = chunk_size - overlap\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += step\n",
    "    return chunks\n",
    "pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"  # replace with your actual path\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "chunks = create_chunks(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):  # just show first 3 chunks\n",
    "    print(f\"--- Chunk {i+1} ---\\n{chunk}\\n\")\n",
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Check if text extraction was successful\n",
    "    if not text:\n",
    "        return \"Failed to extract text from the PDF.\"\n",
    "        \n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "    \n",
    "    # Check if best_chunk is a default message\n",
    "    if best_chunk == \"No relevant information found.\" or not best_chunk:\n",
    "        return best_chunk\n",
    "    \n",
    "    # Simple keyword-based answering approach\n",
    "    question_lower = question.lower()\n",
    "    chunk_lower = best_chunk.lower()\n",
    "import re\n",
    "sentences = re.split(r'[.!?]', best_chunk)\n",
    "relevant_sentences = []\n",
    "    \n",
    "    # Look for sentences that contain keywords from the question\n",
    "    keywords = [word for word in question_lower.split() if len(word) > 3]\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # Check if the sentence contains any keywords from the question\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            relevant_sentences.append(sentence)\n",
    "    \n",
    "    if relevant_sentences:\n",
    "        answer = \" \".join(relevant_sentences)\n",
    "        return f\"Based on the document, I found this information: {answer}\"\n",
    "    else:\n",
    "        # If no specific sentences found, return the best chunk\n",
    "        return f\"I couldn't find a specific answer, but here's the most relevant section from the document:\\n\\n{best_chunk}\"\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cbda022-f90a-4e6f-9f47-b4ac43ab5ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chunk 1 ---\n",
      "9.0 RK 15NOV2021 PROD: Unique CRFs\n",
      "Form: Demographics\n",
      "Generated On: 13 DEC 2021 17:19:35\n",
      "9.0 RK 15NOV2021 PROD\n",
      "(8071)Date of Birth Fixed Unit: \n",
      "dd MMM yyyy\n",
      "Age\n",
      "Sex Male\n",
      "Female\n",
      "Ethnicity Hispanic or Latino\n",
      "Not Hispanic or Latino\n",
      "Unknown\n",
      "Not Applicable\n",
      "Race\n",
      "Asian\n",
      "Black or African American\n",
      "American Indian or Alaska Native\n",
      "Native Hawaiian or Other Pacific Islander\n",
      "White\n",
      "Other\n",
      "Race is 'Other', please specify\n",
      "3 of 499\n",
      "\n",
      "Answer: Based on the document, I found this information: 0 RK 15NOV2021 PROD\n",
      "(8071)Date of Birth Fixed Unit: \n",
      "dd MMM yyyy\n",
      "Age\n",
      "Sex Male\n",
      "Female\n",
      "Ethnicity Hispanic or Latino\n",
      "Not Hispanic or Latino\n",
      "Unknown\n",
      "Not Applicable\n",
      "Race\n",
      "Asian\n",
      "Black or African American\n",
      "American Indian or Alaska Native\n",
      "Native Hawaiian or Other Pacific Islander\n",
      "White\n",
      "Other\n",
      "Race is 'Other', please specify\n",
      "3 of 499\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_file: str) -> str:\n",
    "    reader = PdfReader(pdf_file)\n",
    "    text = \"\" \n",
    "    for page in reader.pages:  # This line was incorrectly indented\n",
    "        if page.extract_text():\n",
    "            text += page.extract_text()\n",
    "    return text  # This line was incorrectly indented\n",
    "\n",
    "pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path) #before you sleep set a alarm to remind yourself \n",
    "#to get rid of ALL of your thoughts fro the REST of your life and put phone in the desk drawers \n",
    "# and ONLY use insta on phone at the gym no where else even at church \n",
    "\n",
    "def create_chunks(text: str, chunk_size: int = 100, overlap: int = 200) -> list[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    step = chunk_size - overlap\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += step\n",
    "    return chunks\n",
    "\n",
    "pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"  # replace with your actual path\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "chunks = create_chunks(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):  # just show first 3 chunks\n",
    "    print(f\"--- Chunk {i+1} ---\\n{chunk}\\n\")\n",
    "\n",
    "# Note: You have this function defined twice, I'm keeping just one instance\n",
    "def find_best_chunk(chunks: list[str], question: str) -> str:\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    question_words = question.lower().split()\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.lower()\n",
    "        score = sum(word in chunk_text for word in question_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk\n",
    "\n",
    "def answer_question_from_pdf(pdf_path: str, question: str) -> str:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Check if text extraction was successful\n",
    "    if not text:\n",
    "        return \"Failed to extract text from the PDF.\"\n",
    "        \n",
    "    chunks = create_chunks(text)\n",
    "    best_chunk = find_best_chunk(chunks, question)\n",
    "    \n",
    "    # Check if best_chunk is a default message\n",
    "    if best_chunk == \"No relevant information found.\" or not best_chunk:\n",
    "        return best_chunk\n",
    "    \n",
    "    # Simple keyword-based answering approach\n",
    "    question_lower = question.lower()\n",
    "    chunk_lower = best_chunk.lower()\n",
    "    \n",
    "    import re  # This import should be at the top of the file, but I'm keeping it here as in your original code\n",
    "    sentences = re.split(r'[.!?]', best_chunk)\n",
    "    relevant_sentences = []\n",
    "    \n",
    "    # Look for sentences that contain keywords from the question\n",
    "    keywords = [word for word in question_lower.split() if len(word) > 3]\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # Check if the sentence contains any keywords from the question\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            relevant_sentences.append(sentence)\n",
    "    \n",
    "    if relevant_sentences:\n",
    "        answer = \" \".join(relevant_sentences)\n",
    "        return f\"Based on the document, I found this information: {answer}\"\n",
    "    else:\n",
    "        # If no specific sentences found, return the best chunk\n",
    "        return f\"I couldn't find a specific answer, but here's the most relevant section from the document:\\n\\n{best_chunk}\"\n",
    "\n",
    "# 5. Run the QA system\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\hydin\\OneDrive\\Desktop\\CRF\\CRFAI.pdf\"\n",
    "    question = \"What is the date of birth format?\"\n",
    "\n",
    "    # Call the function and capture the returned answer\n",
    "    answer = answer_question_from_pdf(pdf_path, question)\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43192649-2e98-4ed4-acff-8a5daf5f40c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9ad4a-0fbf-4887-a623-d7b2aad8c621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
